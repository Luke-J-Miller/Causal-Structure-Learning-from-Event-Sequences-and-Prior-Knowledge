{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Readme  \nThis code is intended for use as a base for building models on.  You should include the data from [Causal Structure Learning from Event Sequences](https://www.kaggle.com/datasets/lukemiller1987/causal-structure-learning-from-event-sequences)\n## Libraries\nThis code base was implemented in Python 3.10.12.  If there is a mismatch, please run:  \n  \n`conda create --name myenv python=3.10.12  \nconda activate myenv  \nconda install jupyter  \njupyter notebook  `\n```\n\n### Install Specific Package Versions\n- Uses `pip` to install specific versions of the following Python packages:\n    - `scipy`: version 1.11.2\n    - `numpy`: version 1.23.5\n    - `pandas`: version 2.0.3\n    - `sklearn`: version 1.2.2\n## Import and Version Check\nImports the installed packages and performs a version check using `assert` statements.\n## Check Python Version\nChecks if the Python version starts with '3.10.12'.\n## Import Standard Libraries\nImports `os`, `pickle`, `json`, `zipfile`, `collections`, and `multiprocessing`, whose versions are tied to the Python version.\n\nEach `assert`statement checks if the current package or Python version matches the expected version. If not, it raises an exception displaying the expected and current versions.","metadata":{}},{"cell_type":"code","source":"# Install specific versions\n!pip install scipy==1.11.2 numpy==1.23.5 pandas==2.0.3 \n\n# Import specific versions\nimport scipy\nassert scipy.__version__ == '1.11.2', f'Expected scipy version 1.11.2, got {scipy.__version__}'\n\nimport numpy as np\nassert np.__version__ == '1.23.5', f'Expected numpy version 1.23.5, got {np.__version__}'\n\nimport pandas as pd\nassert pd.__version__ == '2.0.3', f'Expected pandas version 2.0.3, got {pd.__version__}'\n\nimport sys\nassert sys.version.startswith('3.10.12'), f'Expected Python version 3.10.12, got {sys.version}'\n\nimport sklearn\nassert sklearn.__version__ == '1.2.2', f'Expected sklearn 1.2.2, got {sklearn.__version__}'\n\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# standard libraries and their versions are tied to Python version.\nfrom multiprocessing import Pool\nimport os\nimport json\nimport zipfile\nfrom collections import defaultdict\nimport pickle\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:39:38.084839Z","iopub.execute_input":"2023-09-11T18:39:38.085277Z","iopub.status.idle":"2023-09-11T18:39:49.047762Z","shell.execute_reply.started":"2023-09-11T18:39:38.085240Z","shell.execute_reply":"2023-09-11T18:39:49.046505Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy==1.11.2 in /opt/conda/lib/python3.10/site-packages (1.11.2)\nRequirement already satisfied: numpy==1.23.5 in /opt/conda/lib/python3.10/site-packages (1.23.5)\nRequirement already satisfied: pandas==2.0.3 in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Function Descriptions\n\n### `create_datasets_core_logic(args)`\nThis function is the core logic for creating datasets. It receives a tuple `args` containing the dataset index (`idx`), alarm data (`alarm`), and causal data (`causal`).\n\n#### Parameters:\n- `idx`: Index of the dataset.\n- `alarm`: DataFrame containing alarm data.\n- `causal`: List containing causal information.\n\n#### Process:\n1. `unique_alarm_ids`: Extracts unique alarm IDs.\n2. `dataset_folder`: Creates a new folder for each dataset.\n3. Iterates through time windows, creating subfolders and populating data.\n4. `sparse_matrix`: Constructs a sparse matrix from the data.\n5. Saves the sparse matrix and causal data to disk.\n\n### `create_datasets(dataset_list)`\nManages the parallel execution of `create_datasets_core_logic` for multiple datasets.\n\n#### Parameters:\n- `dataset_list`: List of tuples, each containing alarm data and causal information for a dataset.\n\n#### Process:\n1. Uses a pool of worker threads to execute `create_datasets_core_logic` in parallel if `pool=True`.\n2. Zips the generated datasets into a single `.zip` file.\n\n### Important Variables:\n- `n_alarms`: Number of unique alarms.\n- `data, row_indices, col_indices`: Lists to construct the sparse matrix.\n- `subfolder`: Subfolders to group files.\n\n### File Outputs:\n1. Pickle files for causal data (`*_causal.pkl`).\n2. Compressed `.npz` files for the sparse matrices.\n3. A zipped file `Datasets.zip` containing all datasets.\n\n### Additional Notes:\n- `2**10` and `2**8` are used as constants to define the time windows and device IDs, respectively.\n- Multiprocessing is optional and can be toggled by setting the `pool` variable.\n","metadata":{}},{"cell_type":"code","source":"def create_datasets_core_logic(args):\n    idx, alarm, causal = args\n    unique_alarm_ids = alarm['alarm_id'].unique()\n    n_alarms = len(unique_alarm_ids)\n\n    # Create dataset folders\n    dataset_folder = f\"dataset_{idx}\"\n    os.makedirs(dataset_folder, exist_ok=True)\n\n    for window in range(2**10):\n        data, row_indices, col_indices = [], [], []\n        \n        # Create subfolders for each group of 256 files\n        subfolder = os.path.join(dataset_folder, f\"subfolder_{window // 256}\")\n        os.makedirs(subfolder, exist_ok=True)\n\n        for alarm_id_idx, alarm_id in enumerate(unique_alarm_ids):\n            rows = alarm[alarm['alarm_id'] == alarm_id]\n            \n            for _, row in rows.iterrows():\n                for t in range(row['start_timestamp'], row['end_timestamp']):\n                    if t // 2**10 == window:\n                        data.append(1)\n                        row_indices.append(alarm_id_idx)\n                        col_indices.append(t % 2**10 * 2**8 + row['device_id'])\n        \n        sparse_matrix = csr_matrix((data, (row_indices, col_indices)), shape=(n_alarms, 2**10 * 2**8))\n        \n        with open(f\"{subfolder}/dataset_{idx}_{window}_causal.pkl\", 'wb') as f:\n            pickle.dump([causal[alarm_id] for alarm_id in unique_alarm_ids], f)\n        \n        np.savez(f\"{subfolder}/dataset_{idx}_{window}.npz\", data=sparse_matrix.data, indices=sparse_matrix.indices, indptr=sparse_matrix.indptr, shape=sparse_matrix.shape)\n\ndef create_datasets(dataset_list):\n    pool = True\n    if pool:\n        with Pool() as pool:\n            pool.map(create_datasets_core_logic, [(idx, alarm, causal) for idx, (alarm, causal) in enumerate(dataset_list)])\n    else:\n        for idx, (alarm, causal) in enumerate(dataset_list):\n            create_datasets_core_logic((idx, alarm, causal))\n    \n    # Zip entire datasets folder\n    with zipfile.ZipFile('Datasets.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk('.'):\n            for file_ in files:\n                if file_.endswith('.npz') or file_.endswith('.pkl'):\n                    zipf.write(os.path.join(root, file_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function Description: `generate_alarm_id_mapping(dataset_list)`\n\n### Purpose:\nThis function generates a mapping of unique alarm IDs for each dataset in the given `dataset_list`.\n\n#### Parameters:\n- `dataset_list`: List of tuples, each containing alarm data and causal information for a dataset.\n\n#### Process:\n1. Iterates through each dataset in the `dataset_list`.\n2. `dataset_folder`: Creates a folder for each dataset if it doesn't already exist.\n3. `unique_alarm_ids`: Fetches the unique alarm IDs from the alarm data and sorts them.\n4. `mapping`: Generates a dictionary that maps index to alarm ID.\n5. Saves this mapping as a pickle file (`alarm_id_mapping.pkl`) in the dataset folder.\n\n### Important Variables:\n- `unique_alarm_ids`: Sorted list of unique alarm IDs from the alarm data.\n- `mapping`: Dictionary containing the index-to-alarm ID mapping.\n\n### File Outputs:\n- Pickle file (`alarm_id_mapping.pkl`) containing the index-to-alarm ID mapping for each dataset.\n\n### Additional Notes:\n- This function is typically run before creating datasets to ensure that alarm IDs are consistently mapped across different files and operations.\n","metadata":{}},{"cell_type":"code","source":"def generate_alarm_id_mapping(dataset_list):\n    for idx, (alarm, _) in enumerate(dataset_list):\n        # Create dataset folder if not exists\n        dataset_folder = f\"dataset_{idx}\"\n        os.makedirs(dataset_folder, exist_ok=True)\n        \n        unique_alarm_ids = sorted(alarm['alarm_id'].unique())\n        mapping = {alarm_id_idx: alarm_id for alarm_id_idx, alarm_id in enumerate(unique_alarm_ids)}\n        \n        # Save this mapping\n        with open(f\"{dataset_folder}/alarm_id_mapping.pkl\", 'wb') as f:\n            pickle.dump(mapping, f)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Definition of Source files/ creation of dataset list\n","metadata":{}},{"cell_type":"code","source":"alarm1 = pd.read_csv('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_1/alarm.csv')\ncausal1 = np.load('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_1/causal_prior.npy' , allow_pickle = True)\n\nalarm2 = pd.read_csv('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_2/alarm.csv')\ncausal2 = np.load('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_2/causal_prior.npy' , allow_pickle = True)\n\nalarm3 = pd.read_csv('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_3/alarm.csv')\ncausal3 = np.load('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_3/causal_prior.npy' , allow_pickle = True)\n\nalarm4 = pd.read_csv('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_4/alarm.csv')\ncausal4 = np.load('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_4/causal_prior.npy' , allow_pickle = True)\n\nrca1 = pd.read_csv('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_1/rca_prior.csv')\ntopology1 = np.load('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_1/topology.npy' , allow_pickle = True)\nrca2 = pd.read_csv('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_2/rca_prior.csv')\ntopology2 = np.load('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_2/topology.npy' , allow_pickle = True)\nrca3 = pd.read_csv('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_3/rca_prior.csv')\ntopology3 = np.load('/kaggle/input/causal-structure-learning-from-event-sequences/datasets/dataset_3/topology.npy' , allow_pickle = True)\n\ndataset_list = [(alarm1, causal1), (alarm2, causal2), (alarm3, causal3), (alarm4, causal4)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# run the functions","metadata":{}},{"cell_type":"code","source":"generate_alarm_id_mapping(dataset_list)\ncreate_datasets(dataset_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}